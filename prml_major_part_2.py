# -*- coding: utf-8 -*-
"""PRML_Major_part_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D8akKGFzPz-bSqJckAJ8DA42Gi6B44L5

# Mouting the Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Importing Necessary Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import os
from os import listdir
import cv2

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set()
from PIL import Image
from glob import glob
from skimage.io import imread

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import TensorDataset, DataLoader, Dataset
import torch.optim as optim

import time
import copy
from tqdm import tqdm_notebook as tqdm

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from lightgbm import LGBMClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

print('Imports complete')

base_dir= '/content/drive/MyDrive/Colab Notebooks'
print(os.listdir(base_dir))

!unzip '/content/drive/MyDrive/Colab Notebooks/histopathologic-cancer-detection.zip' > /dev/null

train_path = '/content/train'
test_path = '/content/test'
train_files = os.listdir(train_path)
test_files = os.listdir(test_path)

labels_ = pd.read_csv('/content/train_labels.csv')
labels_.head()

"""# Sampling Random Statifies Data"""

sampled, remainder = train_test_split(labels_, stratify=labels_.label, train_size=0.025)

sampled.to_csv('/content/labels_sampled.csv')

labels = pd.read_csv("/content/labels_sampled.csv")
labels.head()

from PIL import Image
IMG_WIDTH=96
IMG_HEIGHT=96
img_folder='/content/train'

# Commented out IPython magic to ensure Python compatibility.
import random
import matplotlib.image as mpimg
# %matplotlib inline

def create_dataset_PIL(img_folder,labels):

    img_data_array=[]
    class_name=[]

    for i, file in enumerate(labels['id']):

            image= np.array(Image.open("/content/train/" + file + ".tif"))
            image = image.astype('float32')
            image /= 255
            img_data_array.append(image)
            class_name.append(labels['label'][i])
    return img_data_array , class_name


PIL_img_data, class_name=create_dataset_PIL(img_folder,labels)

print(len(PIL_img_data))
print(PIL_img_data[0].shape)

flatten=[]
for i in PIL_img_data:
  flatten.append(i.flatten())
flatten = np.array(flatten)
flatten.shape

"""#Principal Component Analysis



"""

pca90 = PCA(n_components = 0.9)
pca90.fit(flatten,class_name)
pca_data90 = pca90.transform(flatten)

lda = LDA(store_covariance= True)
lda.fit(pca_data90,class_name)
lda_data90 = lda.transform(pca_data90)

"""# **Models without Applying PCA**

## Linear SVC
"""

X_train,X_test,y_train,y_test=train_test_split(flatten,class_name,stratify=class_name,train_size=0.8,random_state=10)

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler


from sklearn.pipeline import make_pipeline


clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)

print(classification_report(y_test, y_pred))

"""# **KNN**"""

from sklearn.neighbors import KNeighborsClassifier


KNN_ = KNeighborsClassifier(n_neighbors=5)
KNN_.fit(X_train, y_train)

KNeighborsClassifier()

y_pred=KNN_.predict(X_test)

print(classification_report(y_test, y_pred))

"""# **Using RBF SVC**"""

from sklearn.svm import SVC


clf = make_pipeline(StandardScaler(), SVC(random_state=0, gamma='auto'))
clf.fit(X_train,y_train)

Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc', SVC(gamma='auto', random_state=0))])

y_pred=clf.predict(X_test)

print(classification_report(y_test, y_pred))

"""# **Using LGBM**"""

lgbm = LGBMClassifier()
lgbm.fit(X_train,y_train)

y_pred = lgbm.predict(X_test)
acc = classification_report(y_test,y_pred)
print(acc)

""".

# **Models with PCA(0.9) and LDA**

### **Linear Support Vector Classifier**
"""

X_train,X_test,y_train,y_test=train_test_split(lda_data90,class_name,stratify=class_name,train_size=0.8,random_state=10)

model = Pipeline((('scaler',StandardScaler()),('linear svc', LinearSVC(random_state=0, tol=1e-5))))
model.fit(X_train,y_train)

y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""**Grid Search**"""

clf = GridSearchCV(model,{'linear svc__C':[0.25,2.5,25,250]})
clf.fit(X_train,y_train)
clf.best_params_

clf.cv_results_

"""## **KNN**"""

from sklearn.neighbors import KNeighborsClassifier


KNN_ = KNeighborsClassifier(n_neighbors=5)
KNN_.fit(X_train, y_train)

KNeighborsClassifier()

y_pred=KNN_.predict(X_test)


print(classification_report(y_test, y_pred))

grid_params = { 'n_neighbors' : [5,7,9,11,13,15],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}


clf = GridSearchCV(KNN_,grid_params)


clf.fit(X_train,y_train)

clf.best_params_

KNN_ = KNeighborsClassifier(metric = 'minkowski',n_neighbors=13,weights='uniform')
KNN_.fit(X_train, y_train)

y_pred=KNN_.predict(X_test)
acc=classification_report(y_test, y_pred)
print(acc)

clf.cv_results_

"""## **RBF SVC**

"""

from sklearn.svm import SVC


model = Pipeline((('scaler',StandardScaler()), ('rbf_svm',SVC(random_state=0, gamma='auto'))))
model.fit(X_train,y_train)


y_pred=model.predict(X_test)

print(classification_report(y_test, y_pred))

"""### **Grid Search**"""

clf = GridSearchCV(model,{'rbf_svm__C': [1,10,100,1000]})


clf.fit(X_train,y_train)

GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                       ('rbf_svm',
                                        SVC(C=1000, gamma='auto',
                                            random_state=0))]),
             param_grid={'rbf_svm__C': [1, 10, 100, 1000]})

clf.best_params_

model = Pipeline((('scaler',StandardScaler()), ('rbf_svm',SVC(random_state=0,C =100, gamma='auto'))))
model.fit(X_train,y_train)

y_pred=model.predict(X_test)
acc=classification_report(y_test, y_pred)
print(acc)

clf.cv_results_



"""# **Light GBM**


"""

lgbm = LGBMClassifier()
model = LGBMClassifier()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)
acc = classification_report(y_test,y_pred)
print(acc)

clf = GridSearchCV(model,dict(num_leaves=[5,10,100,1000],max_depth = [2,3,4,5,6]))

clf.fit(X_train,y_train)

clf.best_params_

model = LGBMClassifier(num_leaves = 100,max_depth = 4)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
acc = classification_report(y_test,y_pred)
print(acc)

clf.cv_results_



X_PIL_train, X_PIL_test, y_PIL_train, y_PIL_test=train_test_split(PIL_img_data ,class_name,stratify=class_name,train_size=0.8,random_state=10)

XX=np.asarray(X_PIL_train)
YY=np.asarray(y_PIL_train)
xx=np.asarray(X_PIL_test)
yy=np.asarray(y_PIL_test)
xx.shape

x1=np.swapaxes(XX,1,3)
x1=np.swapaxes(x1,2,3)
x1.shape

x2=np.swapaxes(xx,1,3)
x2=np.swapaxes(x2,2,3)
x2.shape

class Dataset(Dataset):
    def __init__(self, X,y, transform=None):
        super().__init__()
        self.X = torch.from_numpy(X).float()
        self.y = y
        self.transform = transform

    def __len__(self):
        return len(self.y)

    def __getitem__(self, index):
        image= self.X[index]
        label =self.y[index]
        if self.transform is not None:
            image = self.transform(image)
        return image, label

batch_size = 128
valid_size = 0.1

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_epochs = 3
num_classes = 2
learning_rate = 0.01

def create_dataset_PIL(img_folder,labels):

    img_data_array=[]
    class_name=[]
    for file in range(len(labels)):

            image_path= os.path.join(img_folder,  labels['id'][file]+'.tif')
            image= np.array(Image.open(image_path))
            image = image.astype('float32')
            image /= 255
            img_data_array.append(image)
            class_name.append(labels['label'][file])
    return img_data_array , class_name


PIL_img_data, class_name=create_dataset_PIL(img_folder,labels)



trans_train = transforms.Compose([transforms.ToPILImage(), transforms.Pad(64, padding_mode='reflect'),transforms.RandomHorizontalFlip(),
                                  transforms.RandomVerticalFlip(), transforms.RandomRotation(20),
                                  transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])

trans_valid = transforms.Compose([transforms.ToPILImage(), transforms.Pad(64, padding_mode='reflect'),
                                  transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])

dataset_train = Dataset(X=x1, y=y_PIL_train, transform=trans_train)
dataset_valid = Dataset(X=x2, y=y_PIL_test, transform=trans_valid)

train_loader = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)
valid_loader = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.layer1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, padding=2),
                                    nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer2 = nn.Sequential(nn.Conv2d( 32, 64, kernel_size=3, padding=2),
                    nn.BatchNorm2d(64),nn.ReLU(),nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer3 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, padding=2),
                                nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer4 = nn.Sequential( nn.Conv2d(128, 256, kernel_size=3, padding=2),
                                nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer5 = nn.Sequential( nn.Conv2d(256, 512, kernel_size=3, padding=2),
            nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2))

        self.avg = nn.AvgPool2d(8)
        self.fc = nn.Linear(512 * 1 * 1, 2)

    def forward(self,x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.avg(x)
        x = x.view(-1, 512 * 1 * 1)
        x = self.fc(x)
        return x

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model = CNN().to(device)
print(model)

criterion = nn.CrossEntropyLoss()


optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)


train_losses = []
val_losses = []
train_auc = []
val_auc = []
train_auc_epoch = []
val_auc_epoch = []
best_acc = 0.0
min_loss = np.Inf

since = time.time()

for e in range(num_epochs):

    train_loss = 0.0
    val_loss = 0.0

    model.train()
    for i, (images, labels) in enumerate(tqdm(train_loader, total=int(len(train_loader)))):
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        y_actual = labels.data.cpu().numpy()
        y_pred = outputs[:,-1].detach().cpu().numpy()
        train_auc.append(roc_auc_score(y_actual, y_pred))


    model.eval()
    for i, (images, labels) in enumerate(tqdm(valid_loader, total=int(len(valid_loader)))):
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        loss = criterion(outputs, labels)


        val_loss += loss.item()
        y_actual = labels.data.cpu().numpy()
        y_pred = outputs[:,-1].detach().cpu().numpy()
        val_auc.append(roc_auc_score(y_actual, y_pred))

    train_loss = train_loss/len(train_loader)
    val_loss = val_loss/len(valid_loader)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    training_auc = np.mean(train_auc)
    validation_auc = np.mean(val_auc)
    train_auc_epoch.append(training_auc)
    val_auc_epoch.append(validation_auc)


    if best_acc < validation_auc:
        best_acc = validation_auc

    if min_loss >= val_loss:
        torch.save(model.state_dict(), 'best_model.pt')
        min_loss = val_loss

    print('EPOCH {}/{} Train loss: {:.6f},Validation loss: {:.6f}, Train AUC: {:.4f}  Validation AUC: {:.4f}\n  '.format(e+1, num_epochs,train_loss,val_loss, training_auc,validation_auc))
    print('-' * 10)
time_elapsed = time.time() - since
print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
print('Best validation accuracy: {:4f}'.format(best_acc))



plt.figure(figsize=(20,5))
plt.plot(train_losses, '-o', label="train")
plt.plot(val_losses, '-o', label="val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss change over epoch")
plt.legend()

plt.figure(figsize=(20,5))
plt.plot(train_auc_epoch, '-o', label="train")
plt.plot(val_auc_epoch, '-o', label="val")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy over epoch")
plt.legend()